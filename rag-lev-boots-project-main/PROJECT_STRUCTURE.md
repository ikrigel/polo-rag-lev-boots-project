# LevBoots RAG System - Project Structure

## ğŸ“ Directory Overview

```
polo-rag-lev-boots-project/
â”œâ”€â”€ server/                          # Node.js backend - RAG pipeline
â”‚   â”œâ”€â”€ BusinessLogic/
â”‚   â”‚   â”œâ”€â”€ AskAI.ts                # Core RAG implementation
â”‚   â”‚   â””â”€â”€ LoadDataToAI.ts         # Data loading orchestration
â”‚   â”‚
â”‚   â”œâ”€â”€ services/
â”‚   â”‚   â”œâ”€â”€ embeddings.ts           # Gemini API integration for embeddings
â”‚   â”‚   â”œâ”€â”€ dataLoader.ts           # Main data loading pipeline
â”‚   â”‚   â”œâ”€â”€ pdfLoader.ts            # PDF parsing and extraction
â”‚   â”‚   â”œâ”€â”€ articleLoader.ts        # Article fetching from HTTP
â”‚   â”‚   â”œâ”€â”€ slackLoader.ts          # Slack API integration
â”‚   â”‚   â”œâ”€â”€ chunking.ts             # Text chunking logic
â”‚   â”‚   â””â”€â”€ ragService.ts           # RAG service interface
â”‚   â”‚
â”‚   â”œâ”€â”€ controllers/
â”‚   â”‚   â””â”€â”€ ragController.ts        # Express route handlers
â”‚   â”‚
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â”œâ”€â”€ KnowledgeBase.ts        # Sequelize model for KB table
â”‚   â”‚   â””â”€â”€ index.ts                # Database initialization
â”‚   â”‚
â”‚   â”œâ”€â”€ config/
â”‚   â”‚   â”œâ”€â”€ database.ts             # Database connection setup
â”‚   â”‚   â””â”€â”€ config.cjs              # Sequelize configuration
â”‚   â”‚
â”‚   â”œâ”€â”€ utils/
â”‚   â”‚   â””â”€â”€ logger.ts               # Logging utility
â”‚   â”‚
â”‚   â”œâ”€â”€ routes/
â”‚   â”‚   â””â”€â”€ ragRoutes.ts            # Express routes
â”‚   â”‚
â”‚   â”œâ”€â”€ migrations/
â”‚   â”‚   â”œâ”€â”€ 20250927100114-enable-pgvector-extension.cjs
â”‚   â”‚   â””â”€â”€ 20250927100126-create-embeddings-table.cjs
â”‚   â”‚
â”‚   â”œâ”€â”€ knowledge_pdfs/             # Source PDFs
â”‚   â”‚   â”œâ”€â”€ OpEd - A Revolution at Our Feet.pdf
â”‚   â”‚   â”œâ”€â”€ Research Paper - Gravitational Reversal Physics.pdf
â”‚   â”‚   â””â”€â”€ White Paper - The Development of Localized Gravity Reversal Technology.pdf
â”‚   â”‚
â”‚   â”œâ”€â”€ server.ts                   # Express server entry point
â”‚   â”œâ”€â”€ package.json                # Backend dependencies
â”‚   â”œâ”€â”€ tsconfig.json               # TypeScript config
â”‚   â”œâ”€â”€ .env.example                # Environment template
â”‚   â””â”€â”€ dist/                        # Compiled JavaScript (generated)
â”‚
â”œâ”€â”€ public/                          # React frontend - Vite
â”‚   â”œâ”€â”€ src/
â”‚   â”‚   â”œâ”€â”€ components/
â”‚   â”‚   â”‚   â””â”€â”€ Home.tsx            # Main RAG interface component
â”‚   â”‚   â”œâ”€â”€ App.tsx                 # Root component
â”‚   â”‚   â”œâ”€â”€ main.tsx                # React entry point
â”‚   â”‚   â”œâ”€â”€ index.css               # Global styles
â”‚   â”‚   â””â”€â”€ vite-env.d.ts           # Vite type definitions
â”‚   â”‚
â”‚   â”œâ”€â”€ vite.config.ts              # Vite configuration (API proxy)
â”‚   â”œâ”€â”€ package.json                # Frontend dependencies
â”‚   â”œâ”€â”€ tsconfig.json               # TypeScript config
â”‚   â”œâ”€â”€ index.html                  # HTML entry point
â”‚   â””â”€â”€ dist/                        # Built frontend (generated)
â”‚
â”œâ”€â”€ logs/                            # Application logs
â”‚   â””â”€â”€ server.log                   # Consolidated server logs
â”‚
â”œâ”€â”€ .gitignore                       # Git exclusion rules
â”œâ”€â”€ PROJECT_STRUCTURE.md             # This file
â”œâ”€â”€ HOW_TO_USE.md                    # Usage instructions
â”œâ”€â”€ EXTRA_CHALLENGES.md              # Additional challenges
â”œâ”€â”€ README.md                        # Original project spec
â””â”€â”€ package.json                     # Root package.json


```

---

## ğŸ—ï¸ Architecture Overview

### Three-Tier Architecture

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                    React Frontend                        â”‚
â”‚              (Vite + Mantine UI Components)             â”‚
â”‚  - Question input                                        â”‚
â”‚  - Load Knowledge Base button                            â”‚
â”‚  - Answer display with Bibliography                      â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚ HTTP
                       â”‚ (API proxy: localhost:5173 â†’ 3030)
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                  Express Backend                         â”‚
â”‚              (/api routes on port 3030)                 â”‚
â”‚  - /api/ask           â†’ Ask question & get answer        â”‚
â”‚  - /api/load_data     â†’ Load KB from sources             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                       â”‚
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚             Postgres Database (Supabase)                 â”‚
â”‚              knowledge_base table with:                  â”‚
â”‚  - chunk_content      â†’ Text chunks from sources         â”‚
â”‚  - embeddings_768     â†’ Vector embeddings (768-dim)      â”‚
â”‚  - source             â†’ Document name/URL                â”‚
â”‚  - source_id          â†’ Unique chunk identifier          â”‚
â”‚  - chunk_index        â†’ Position in document             â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## ğŸ”„ Data Flow: Question to Answer

### Step 1: Load Knowledge Base
```
Sources (PDFs, Articles, Slack)
    â†“
[pdfLoader.ts] [articleLoader.ts] [slackLoader.ts]
    â†“
[chunking.ts] â†’ Split into ~400 word chunks
    â†“
[embeddings.ts] â†’ Generate 768-dim vectors (Gemini API)
    â†“
[dataLoader.ts] â†’ Batch insert to knowledge_base table
    â†“
PostgreSQL knowledge_base table
```

### Step 2: Ask a Question
```
User Question
    â†“
[Home.tsx] â†’ User submits question via UI
    â†“
/api/ask endpoint
    â†“
[ragController.ts] â†’ Route handler
    â†“
[ragService.ts] â†’ Ask function
    â†“
[AskAI.ts] â†’ RAG implementation:
    1. Embed question (Gemini API) â†’ 768-dim vector
    2. Similarity search (cosine) â†’ Find top 5 chunks
    3. Construct system prompt with retrieved context
    4. Call Perplexity API (sonar model) â†’ Generate answer
    5. Extract unique sources
    6. Return: { answer, sources, bibliography }
    â†“
[Home.tsx] â†’ Display answer + bibliography
```

---

## ğŸ“¦ Key Files & Responsibilities

### Backend Core Files

#### `server/server.ts`
- Express app initialization
- Loads environment variables via `dotenv`
- Serves static frontend files
- Initializes database migrations

#### `server/BusinessLogic/AskAI.ts`
**Implements the RAG pipeline:**
- `askAI(userQuestion)` - Main RAG function
- `constructSystemPrompt()` - Creates LLM system prompt with context
- `retrieveRelevantChunks()` - Vector similarity search
- `cosineSimilarity()` - Calculates similarity between embeddings
- Constants:
  - `SIMILARITY_THRESHOLD = 0.3` - Minimum relevance score
  - `TOP_K = 5` - Number of chunks to retrieve
  - `PERPLEXITY_MODEL = 'sonar'` - LLM model name

#### `server/services/embeddings.ts`
**Handles Gemini embeddings:**
- `generateEmbeddings()` - Batch embedding generation
- `generateEmbedding()` - Single embedding
- `getEmbeddingDimension()` - Returns 768
- Features:
  - Retry logic with exponential backoff
  - 30-second timeout per API call
  - Rate limiting (1 second delay between requests)

#### `server/services/dataLoader.ts`
**Orchestrates the data loading pipeline:**
- Step 1: Load PDFs, articles, Slack messages
- Step 2: Chunk all content
- Step 3: Check for existing data (avoid duplicates)
- Step 4: Generate embeddings for all chunks
- Step 5: Store to database in batches (50 chunks per batch)

#### `server/models/KnowledgeBase.ts`
**Database model for chunks:**
```typescript
{
  source: string              // Document name
  source_id: string           // Unique chunk ID
  chunk_index: number         // Position in document
  chunk_content: string       // Actual text
  embeddings_768: number[]    // Vector representation
  embeddings_1536?: number[]  // Alternative embedding dimension
}
```

### Frontend Core Files

#### `public/src/components/Home.tsx`
**Main RAG UI component:**
- State management:
  - `question` - User input
  - `response` - API response (with answer, sources, bibliography)
  - `isLoading` - Loading indicator
  - `error` - Error message
  - `isDataLoaded` - KB loaded flag
- Functions:
  - `loadData()` - Call /api/load_data endpoint
  - `submitQuestion()` - Send question to /api/ask
  - `checkDataStatus()` - Verify KB is loaded
- Display:
  - Answer section (clean text, no inline citations)
  - Bibliography section (numbered list of sources)

#### `public/vite.config.ts`
**Frontend build & proxy config:**
```typescript
proxy: {
  '/api': {
    target: 'http://localhost:3030',  // Backend URL
    changeOrigin: true,
  },
}
```

---

## ğŸ” Security & Environment Variables

### Required Environment Variables
Create `server/.env` (use `server/.env.example` as template):
```
PERPLEXITY_API_KEY=your_api_key_here
Gemini_API_KEY=your_api_key_here
PERPLEXITY_MODEL=sonar
DATABASE_URL=postgresql://user:pass@host:port/db
```

### Secrets Protection
- `.env` file is in `.gitignore` - never committed to GitHub
- `.env.example` shows required variables without values
- API keys safely stored locally only

---

## ğŸ—„ï¸ Database Schema

### `knowledge_base` Table
```sql
CREATE TABLE knowledge_base (
  id SERIAL PRIMARY KEY,
  source VARCHAR(255),           -- Document source name
  source_id VARCHAR(255),        -- Unique chunk identifier
  chunk_index INTEGER,           -- Position in document
  chunk_content TEXT,            -- Text content (~400 words)
  embeddings_768 FLOAT8[],       -- Vector embeddings (768 dimensions)
  embeddings_1536 FLOAT8[],      -- Alternative embedding (1536 dimensions)
  created_at TIMESTAMP DEFAULT NOW(),
  updated_at TIMESTAMP DEFAULT NOW()
);

-- Enable pgvector for similarity search
CREATE EXTENSION IF NOT EXISTS vector;
CREATE INDEX ON knowledge_base USING ivfflat (embeddings_768 vector_cosine_ops);
```

---

## ğŸš€ API Endpoints

### POST `/api/ask`
**Ask a question and get RAG answer**

Request:
```json
{
  "userQuestion": "How do levboots work?"
}
```

Response:
```json
{
  "answer": "LevBoots work by using an Aetheric Field Generator...",
  "sources": [
    "White Paper - The Development of Localized Gravity Reversal Technology.pdf",
    "OpEd - A Revolution at Our Feet.pdf"
  ],
  "bibliography": [
    "White Paper - The Development of Localized Gravity Reversal Technology.pdf",
    "OpEd - A Revolution at Our Feet.pdf"
  ]
}
```

### POST `/api/load_data`
**Load and embed all knowledge base sources**

Request: (no body required)

Response:
```json
{
  "ok": true,
  "message": "Data loaded successfully"
}
```

---

## ğŸ“Š Configuration Constants

### Similarity Search
| Constant | Value | Purpose |
|----------|-------|---------|
| `SIMILARITY_THRESHOLD` | 0.3 | Min relevance score to include chunk |
| `TOP_K` | 5 | Number of top chunks to retrieve |
| `EMBEDDING_DIMENSION` | 768 | Vector dimensions |

### API Configuration
| Service | Model | Details |
|---------|-------|---------|
| Embeddings | Gemini text-embedding-004 | 768-dimensional vectors |
| LLM Answer | Perplexity sonar | Latest sonar model (replaces llama-3.1) |
| Database | Postgres + pgvector | Similarity search via cosine distance |

### Performance & Reliability
| Setting | Value | Purpose |
|---------|-------|---------|
| Request Timeout | 30 seconds | Per API call timeout |
| Max Retries | 3 attempts | Exponential backoff: 1s, 2s, 4s |
| Batch Size | 50 chunks | Database insert batch size |
| Delay Between Requests | 1 second | Rate limiting for embeddings |

---

## ğŸ”„ Request Flow Diagram

```
Frontend (React)
    â”‚
    â”œâ”€ User clicks "Load Knowledge Base"
    â”‚   â””â”€â†’ POST /api/load_data
    â”‚       â”œâ”€ Load PDFs, Articles, Slack
    â”‚       â”œâ”€ Chunk content (400 words each)
    â”‚       â”œâ”€ Generate embeddings (Gemini)
    â”‚       â””â”€ Insert to DB
    â”‚
    â””â”€ User types question & submits
        â””â”€â†’ POST /api/ask { userQuestion: "..." }
            â”œâ”€ Embed question (Gemini)
            â”œâ”€ Vector similarity search (cosine)
            â”œâ”€ Retrieve top 5 chunks
            â”œâ”€ Build system prompt with context
            â”œâ”€ Call Perplexity LLM (sonar)
            â”œâ”€ Extract unique sources
            â””â”€â†’ Return { answer, sources, bibliography }

Frontend displays:
- Answer section (first)
- Bibliography section (numbered list below)
```

---

## ğŸ“š Data Sources

### PDFs (3 documents)
- `server/knowledge_pdfs/OpEd - A Revolution at Our Feet.pdf`
- `server/knowledge_pdfs/Research Paper - Gravitational Reversal Physics.pdf`
- `server/knowledge_pdfs/White Paper - The Development of Localized Gravity Reversal Technology.pdf`

### Articles (5 articles from HTTP)
- military-deployment-report
- urban-commuting
- hover-polo
- warehousing
- consumer-safety

### Slack API (Simulated)
Three channels with paginated results:
- lab-notes
- engineering
- offtopic

---

## ğŸ› ï¸ Technology Stack

### Backend
- **Runtime**: Node.js
- **Framework**: Express.js
- **Language**: TypeScript
- **Database**: PostgreSQL with pgvector
- **ORM**: Sequelize
- **APIs**:
  - Google Gemini (embeddings)
  - Perplexity (LLM answers)
- **HTTP Client**: Axios
- **Logging**: Custom logger to file + console

### Frontend
- **Framework**: React 19
- **Build Tool**: Vite
- **UI Library**: Mantine v8
- **Icons**: Tabler Icons
- **State Management**: MobX
- **Styling**: CSS-in-JS (Mantine theming)

### Infrastructure
- **Database**: Supabase (PostgreSQL + pgvector)
- **Port Assignment**:
  - Backend: 3030
  - Frontend Dev: 5173
  - Frontend Build: Static files in `/public/dist`

---

## ğŸ“– Key Concepts

### Retrieval-Augmented Generation (RAG)
1. **Retrieval**: Find relevant documents based on query
2. **Augmentation**: Use retrieved docs to build context
3. **Generation**: LLM generates answer based on context

### Vector Embeddings
- Text converted to 768-dimensional vectors
- Vectors capture semantic meaning
- Similar text = similar vectors
- Cosine similarity measures proximity

### Chunking
- Large documents split into ~400 word chunks
- Preserves context while limiting token count
- Each chunk independently embedded

### Similarity Search
- Use cosine distance to find relevant chunks
- Formula: `(AÂ·B) / (||A|| Ã— ||B||)`
- Threshold 0.3 filters irrelevant results

---

## ğŸš¦ Status Indicators

### Logging Levels
| Symbol | Level | Meaning |
|--------|-------|---------|
| â„¹ï¸ | INFO | Important operations |
| ğŸ” | DEBUG | Detailed debugging info |
| âš ï¸ | WARN | Warning conditions |
| âŒ | ERROR | Error occurred |
| âœ“ | SUCCESS | Operation completed |

All logs written to `logs/server.log` and console.

---

## ğŸ”§ Running the Project

### Development
```bash
# Install dependencies
npm install

# Start backend (port 3030)
cd server && npm start

# Start frontend (port 5173) - in new terminal
cd public && npm run dev

# Open browser: http://localhost:5173
```

### Build for Production
```bash
# Build backend
cd server && npm run build

# Build frontend
cd public && npm run build

# Serve static frontend from backend
npm start (from server directory)
```

---

## ğŸ“ Notes

- **First-time setup**: Click "Load Knowledge Base" to populate the database
- **Rate limiting**: API calls have delays to respect rate limits
- **Vector dimension**: Entire project uses 768-dimensional embeddings consistently
- **Model migration**: Updated from deprecated `llama-3.1-sonar-*` to current `sonar` model
- **Environment variables**: Ensure `.env` is created before starting server
